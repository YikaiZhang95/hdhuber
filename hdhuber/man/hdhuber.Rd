% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/huber.R
\name{hdhuber}
\alias{hdhuber}
\title{Solve the huber regression. The solution path is computed
at a grid of values of tuning parameter \code{lambda}.}
\usage{
hdhuber(
  x,
  y,
  nlambda = 100,
  lambda.factor = ifelse(nobs < nvars, 0.01, 1e-04),
  lambda = NULL,
  lam2 = 0,
  delta = 1,
  pf = rep(1, nvars),
  pf2 = rep(1, nvars),
  exclude,
  dfmax = nvars + 1,
  pmax = min(dfmax * 1.2, nvars),
  standardize = TRUE,
  eps = 1e-08,
  maxit = 1e+06,
  is_strong = TRUE
)
}
\arguments{
\item{x}{Matrix of predictors, of dimension \eqn{n * p};
each row is an observation.}

\item{y}{Response variable. The length is \eqn{n}.}

\item{nlambda}{The number of \code{lambda} values (default is 100).}

\item{lambda.factor}{The factor for getting the minimal value
in the \code{lambda} sequence, where
\code{min(lambda)} = \code{lambda.factor} * \code{max(lambda)}
and \code{max(lambda)} is the smallest value of \code{lambda}
for which all coefficients (except the intercept when it is present)
are penalized to zero. The default depends on the relationship
between \eqn{n} (the number of rows in the design matrix) and
\eqn{p} (the number of predictors). If \eqn{n < p}, it defaults to
\code{0.05}. If \eqn{n > p}, the default is \code{0.001},
closer to zero.  A very small value of \code{lambda.factor} will
lead to a saturated fit. The argument takes no effect if there is a
user-supplied \code{lambda} sequence.}

\item{lambda}{A user-supplied \code{lambda} sequence. Typically,
by leaving this option unspecified, users can have the program
compute its own \code{lambda} sequence based on \code{nlambda}
and \code{lambda.factor}. It is better to supply, if necessary,
a decreasing sequence of \code{lambda} values than a single
(small) value. The program will ensure that the user-supplied
\code{lambda} sequence is sorted in decreasing order before
fitting the model to take advanage of the warm-start technique.}

\item{lam2}{Regularization parameter \code{lambda2} for the
quadratic penalty of the coefficients. Unlike \code{lambda},
only one value of \code{lambda2} is used for each fitting process.}

\item{delta}{The smoothing parameter. Default is 0.125.}

\item{pf}{L1 penalty factor of length \eqn{p} used for the adaptive
LASSO or adaptive elastic net. Separate L1 penalty weights can be
applied to each coefficient to allow different L1 shrinkage.
Can be 0 for some variables (but not all), which imposes no
shrinkage, and results in that variable always being included
in the model. Default is 1 for all variables (and implicitly
infinity for variables in the \code{exclude} list).}

\item{pf2}{L2 penalty factor of length \eqn{p} used for adaptive
elastic net. Separate L2 penalty weights can be applied to
each coefficient to allow different L2 shrinkage.
Can be 0 for some variables, which imposes no shrinkage.
Default is 1 for all variables.}

\item{exclude}{Indices of variables to be excluded from the model.
Default is none. Equivalent to an infinite penalty factor.}

\item{dfmax}{The maximum number of variables allowed in the model.
Useful for very large \eqn{p} when a partial path is desired.
Default is \eqn{p+1}.}

\item{pmax}{The maximum number of coefficients allowed ever
to be nonzero along the solution path. For example, once
\eqn{\beta} enters the model, no matter how many times it
exits or re-enters the model through the path, it will be
counted only once. Default is \code{min(dfmax x 1.2, p)}.}

\item{standardize}{Logical flag for variable standardization,
prior to fitting the model sequence. The coefficients are
always returned to the original scale. Default is \code{TRUE}.}

\item{eps}{Stopping criterion.}

\item{maxit}{Maximum number of iterates.}

\item{is_strong}{Use strong rule or not. Default is \code{TRUE}.}
}
\value{
An object with S3 class \code{hdhuber}
\item{alpha}{An \eqn{n+1} by \eqn{L} matrix of coefficients, where \eqn{n} is the number of observations
and \eqn{L} is the number of tuning parameters. The first row of \code{alpha} contains the intercepts.}
\item{lambda}{The \code{lambda} sequence that was actually used.}
\item{npass}{The total number of iterates used to train the classifier.}
\item{jerr}{Warnings and errors; 0 if none.}
\item{info}{A list includes some settings used to fit this object: \code{eps}, \code{maxit}}.
}
\description{
Solve the huber regression. The solution path is computed
at a grid of values of tuning parameter \code{lambda}.
}
\details{
The function implements an accelerated coor descent to solve
huber regression.
}
\keyword{huber}
\keyword{regression}
